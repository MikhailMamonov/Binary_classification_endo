{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains model\n",
    "\n",
    "Usage: python train.py [-h]\n",
    "\"\"\"\n",
    "from argparse import ArgumentParser\n",
    "from multiprocessing import cpu_count\n",
    "from os import path, environ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from utils import (TEST_DATA_PATH, TRAIN_DATA_PATH, VALIDATION_DATA_PATH,\n",
    "                   MODELS_PATH, CLASSES, try_makedirs, plot_loss_acc,\n",
    "                   plot_confusion_matrix)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from models import get_model\n",
    "from config import config\n",
    "\n",
    "\n",
    "def init_argparse():\n",
    "    \"\"\"\n",
    "    Initializes argparse\n",
    "\n",
    "    Returns parser\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser(description='Trains toxic comment classifier')\n",
    "    parser.add_argument(\n",
    "        '-m',\n",
    "        '--model',\n",
    "        nargs='?',\n",
    "        help='model architecture (vgg16, vgg19, incresnet, incv3, xcept, resnet50, densnet, nasnet)',\n",
    "        default='vgg16',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--gpus',\n",
    "        nargs='?',\n",
    "        help=\"A list of GPU device numbers ('1', '1,2,5')\",\n",
    "        default=0,\n",
    "        type=str)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def train_and_predict(model_type, gpus):\n",
    "    \"\"\"\n",
    "    Trains model and makes predictions file\n",
    "    \"\"\"\n",
    "    # creating data generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255, horizontal_flip=True)\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DATA_PATH,\n",
    "        class_mode='binary',\n",
    "        seed=171717,\n",
    "        **config[model_type]['flow_generator'])\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        VALIDATION_DATA_PATH,\n",
    "        class_mode='binary',\n",
    "        \n",
    "        **config[model_type]['flow_generator'])\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        TEST_DATA_PATH,\n",
    "        class_mode=None,\n",
    "        classes=CLASSES,\n",
    "        shuffle=False,\n",
    "        **config[model_type]['flow_generator'])\n",
    "\n",
    "    # loading the model\n",
    "    parallel_model, model = get_model(model=model_type, gpus=gpus)\n",
    "    print('Training model')\n",
    "    print(model.summary())\n",
    "    history = parallel_model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', min_delta=0, patience=5),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.2, patience=3, min_lr=0.000001),\n",
    "            TerminateOnNaN()\n",
    "        ],\n",
    "        max_queue_size=100,\n",
    "        use_multiprocessing=True,\n",
    "        workers=cpu_count(),\n",
    "        **config[model_type]['fit_generator'])\n",
    "    # history of training\n",
    "    # print(history.history.keys())\n",
    "    # Saving architecture + weights + optimizer state\n",
    "    model_path = path.join(MODELS_PATH, '{}_{:.4f}_{:.4f}'.format(\n",
    "        model_type, history.history['val_loss'][-1]\n",
    "        if 'val_loss' in history.history else history.history['loss'][-1],\n",
    "        history.history['val_acc'][-1]\n",
    "        if 'val_acc' in history.history else history.history['acc'][-1]))\n",
    "    try_makedirs(model_path)\n",
    "    plot_model(model, path.join(model_path, 'model.png'), show_shapes=True)\n",
    "    plot_loss_acc(history, model_path)\n",
    "\n",
    "    print('Saving model')\n",
    "    model.save(path.join(model_path, 'model.h5'))\n",
    "    # Building confusion matrices for every class for validation data\n",
    "    print(\"Building confusion matrices\")\n",
    "    val_preds = model.predict_generator(\n",
    "        validation_generator,\n",
    "        max_queue_size=100,\n",
    "        use_multiprocessing=True,\n",
    "        workers=cpu_count())\n",
    "    plot_confusion_matrix(\n",
    "        confusion_matrix(\n",
    "            list(validation_generator.classes), np.argmax(val_preds, axis=1)),\n",
    "        CLASSES, model_path)\n",
    "\n",
    "    print('Generating predictions')\n",
    "    predictions = model.predict_generator(\n",
    "        test_generator,\n",
    "        max_queue_size=100,\n",
    "        use_multiprocessing=True,\n",
    "        workers=cpu_count())\n",
    "    pred_classes = np.argmax(predictions)\n",
    "    # Dealing with missing data\n",
    "    ids = list(map(lambda id: id[5:-4], test_generator.filenames))\n",
    "    proba = predictions[np.arange(len(predictions)), pred_classes]\n",
    "    # Generating predictions.csv for Kaggle\n",
    "    pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'predicted': pred_classes,\n",
    "    }).sort_values(by='id').to_csv(\n",
    "        path.join(model_path, 'predictions.csv'), index=False)\n",
    "    # Generating predictions.csv with some additional data for post-processing\n",
    "    pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'predicted': pred_classes,\n",
    "        'proba': proba\n",
    "    }).sort_values(by='id').to_csv(\n",
    "        path.join(model_path, 'predictions_extd.csv'), index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    args = init_argparse().parse_args()\n",
    "\n",
    "    environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n",
    "\n",
    "    train_and_predict(args.model, args.gpus)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some useful utilities\n",
    "\"\"\"\n",
    "\n",
    "from itertools import product\n",
    "from os import path, makedirs\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "# generates images without having a window appear\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pylab as plt\n",
    "\"\"\"\n",
    "Absolute utils.py file path. It is considered as the project root path.\n",
    "\"\"\"\n",
    "CWD = path.dirname(path.realpath(__file__))\n",
    "\"\"\"\n",
    "It must contain files with raw data\n",
    "\"\"\"\n",
    "DATA_PATH = path.join(CWD, 'data')\n",
    "TEST_DATA_PATH = path.join(DATA_PATH, 'test')\n",
    "TRAIN_DATA_PATH = path.join(DATA_PATH, 'train')\n",
    "VALIDATION_DATA_PATH = path.join(DATA_PATH, 'validation')\n",
    "\n",
    "LOG_PATH = path.join(CWD, 'log')\n",
    "\"\"\"\n",
    "Trained models must be stored here\n",
    "\"\"\"\n",
    "MODELS_PATH = path.join(CWD, 'models')\n",
    "\"\"\"\n",
    "Pickled objects must be stored here\n",
    "\"\"\"\n",
    "PICKLES_PATH = path.join(CWD, 'pickles')\n",
    "CLASSES = list(map(str, ['irreg','norma']))\n",
    "\n",
    "\n",
    "def try_makedirs(name):\n",
    "    \"\"\"\n",
    "    Makes path if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not path.exists(name):\n",
    "            # Strange, but it may raise winerror 123\n",
    "            makedirs(name)\n",
    "    except OSError:\n",
    "        return\n",
    "\n",
    "\n",
    "def plot_loss_acc(history, model_path):\n",
    "    \"\"\"\n",
    "    Saves into files accuracy and loss plots\n",
    "    \"\"\"\n",
    "    plt.gcf().clear()\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(path.join(model_path, 'accuracy.png'))\n",
    "    plt.gcf().clear()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(path.join(model_path, 'loss.png'))\n",
    "    plt.gcf().clear()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_path, title='Confusion matrix'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.gcf().clear()\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='none')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=2)\n",
    "    plt.yticks(tick_marks, classes, fontsize=2)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(False)\n",
    "    plt.savefig(path.join(model_path, 'confusion_matrix.pdf'), format='pdf')\n",
    "\n",
    "    plt.gcf().clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, Convolution2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop,SGD, Adagrad, Adam\n",
    "from keras.utils import multi_gpu_model\n",
    "from utils import CLASSES\n",
    "\n",
    "\n",
    "def get_gpus(gpus):\n",
    "    return list(map(int, gpus.split(',')))\n",
    "\n",
    "\n",
    "def get_model(model, gpus=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns compiled keras parallel model ready for training\n",
    "    and base model that must be used for saving weights\n",
    "\n",
    "    Params:\n",
    "    - model - model type\n",
    "    - gpus - a list with numbers of GPUs\n",
    "    \"\"\"\n",
    "    if model == 'vgg16' or model == 'vgg19':\n",
    "        return vgg(gpus, model)\n",
    "    if model == 'skin_rec':\n",
    "        return skin_rec(gpus, model)\n",
    "    if model == 'lung_rec':\n",
    "        return lung_rec(gpus, model)\n",
    "    if model == 'alex_net':\n",
    "        return alex_net(gpus, model)\n",
    "    if model == 'incresnet':\n",
    "        return inception_res_net_v2(gpus)\n",
    "    if model == 'incv3':\n",
    "        return inception_v3(gpus)\n",
    "    if model == 'xcept':\n",
    "        return xception(gpus)\n",
    "    if model == 'resnet50':\n",
    "        return resnet50(gpus)\n",
    "    if model == 'densenet':\n",
    "        return dense_net(gpus)\n",
    "    if model == 'nasnet':\n",
    "        return nasnet(gpus)\n",
    "    raise ValueError('Wrong model value!')\n",
    "\n",
    "def alex_net(gpus,model):\n",
    "    frozen = 0\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape = (141, 141, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Layer \n",
    "    model.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "    # Layer 6\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Layer 7\n",
    "    model.add(Dense(output_dim = 64, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Layer 8\n",
    "    \n",
    "    output = Dense(len(CLASSES), init='glorot_normal',activation='softmax')(model.output)\n",
    "    \n",
    "    return _compile(gpus, model.input, output, frozen)\n",
    "\n",
    "def vgg(gpus, model):\n",
    "    \"\"\"\n",
    "    Returns compiled keras vgg16 model ready for training\n",
    "    \"\"\"\n",
    "\n",
    "    gpu = get_gpus(gpus)\n",
    "    if model == 'vgg16':\n",
    "        \n",
    "\n",
    "        base_model = VGG16(\n",
    "            weights= 'imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        frozen = 14\n",
    "    elif model == 'vgg19':\n",
    "        base_model = VGG19(\n",
    "            weights= None, include_top=False, input_shape=(224, 224, 3))\n",
    "        frozen = 16\n",
    "    else:\n",
    "        raise ValueError('Wrong VGG model type!')\n",
    "    x = Flatten(name='flatten')(base_model.output)\n",
    "    x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(len(CLASSES), activation='softmax')(x)\n",
    "\n",
    "    # x = Flatten(name='flatten')(base_model.output)\n",
    "    # x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    # x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    # output = Dense(1, activation='sigmoid')(x)\n",
    "    return _compile(gpus, base_model.input, output, 0)\n",
    "\n",
    "def skin_rec(gpus, model):\n",
    "    nb_filters = 64\n",
    "    k_size = (3, 3)\n",
    "    pl_size = (2, 2)\n",
    "    gpu = get_gpus(gpus)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, kernel_size=k_size, activation='relu', input_shape=(141, 141, 3)))\n",
    "    model.add(Conv2D(nb_filters-4, k_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(nb_filters-8, kernel_size=k_size, activation='relu'))\n",
    "    model.add(Conv2D(nb_filters-12, kernel_size=k_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(nb_filters-16, kernel_size=k_size, activation='relu'))\n",
    "    model.add(Conv2D(nb_filters-20, kernel_size=k_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))\t)\n",
    "\n",
    "\t\n",
    "    x = Flatten(name='flatten')(model.output) \n",
    "    x = Dense(128, activation='relu', name='fc1')(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\t      \n",
    "    print('Model flattened out to ', model.output_shape) \n",
    "    print(type(gpus))\n",
    "    print(type(model.input))\n",
    "    print(type(output))\n",
    "    return _compile(gpus, model.input, output, 0)\n",
    "\n",
    "def lung_rec(gpus, model):\n",
    "    k_size = (3, 3)\n",
    "    pl_size = (2, 2)\n",
    "    gpu = get_gpus(gpus)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(50, kernel_size=(11,11), activation='relu', input_shape=(141, 141, 3)))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(120, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    x = Flatten(name='flatten')(model.output) \n",
    "    x = Dense(10, activation='relu', name='fc1')(x)\n",
    "\n",
    "    output = Dense(len(CLASSES), activation='softmax')(x)\n",
    "\n",
    "\t      \n",
    "    print('Model flattened out to ', model.output_shape) \n",
    "    print(type(gpus))\n",
    "    print(type(model.input))\n",
    "    print(type(output))\n",
    "    return _compile(gpus, model.input, output, 0)\n",
    "\n",
    "\n",
    "\n",
    "def inception_v3(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 29\n",
    "\tbase_model = InceptionV3(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(141, 141, 3))\n",
    "\n",
    "\tx = GlobalAveragePooling2D()(base_model.output)\n",
    "\tx = Dense(1024, activation='relu')(x)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def inception_res_net_v2(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 0  # TODO\n",
    "\tbase_model = InceptionResNetV2(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "\tx = GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def xception(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 125\n",
    "\tbase_model = Xception(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "\tx = GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n",
    "\tx = Dense(1024, activation='relu')(x)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def resnet50(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 0\n",
    "\tbase_model = ResNet50(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tx = Flatten()(base_model.output)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def dense_net(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 0\n",
    "\tbase_model = DenseNet201(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\tx = GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def nasnet(gpus):\n",
    "\t\"\"\"\n",
    "\tReturns compiled keras vgg16 model ready for training\n",
    "\t\"\"\"\n",
    "\tfrozen = 0\n",
    "\tbase_model = NASNetLarge(\n",
    "\t\tweights='imagenet', include_top=False, input_shape=(331, 331, 3))\n",
    "\n",
    "\tx = GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n",
    "\toutput = Dense(len(CLASSES), activation='softmax', name='predictions')(x)\n",
    "\n",
    "\treturn _compile(gpus, base_model.input, output, frozen)\n",
    "\n",
    "\n",
    "def _compile(gpus, input, output, frozen):\n",
    "\tgpus = get_gpus(gpus)\n",
    "\tif len(gpus) == 1:\n",
    "\t\twith K.tf.device('/gpu:{}'.format(gpus[0])):\n",
    "\t\t\tmodel = Model(input, output)\n",
    "\t\t\tfor layer in model.layers[:frozen]:\n",
    "\t\t\t\tlayer.trainable = False\n",
    "\t\t\tparallel_model = model\n",
    "\telse:\n",
    "\t\twith K.tf.device('/cpu:0'):\n",
    "\t\t\tmodel = Model(input, output)\n",
    "\t\t\tfor layer in model.layers[:frozen]:\n",
    "\t\t\t\tlayer.trainable = False\n",
    "\t\tparallel_model = multi_gpu_model(model, gpus=gpus)\n",
    "\tparallel_model.compile(\n",
    "\t\tloss='binary_crossentropy',\n",
    "\t\toptimizer='rmsprop',\n",
    "\t\tmetrics=['accuracy'])\n",
    "\treturn parallel_model, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3.5\n",
    "# -*- coding:utf-8 -*-\n",
    "# Images that already exist will not be downloaded again, so the script can\n",
    "# resume a partially completed download. All images will be saved in the JPG\n",
    "# format with 90% compression quality.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "import urllib3\n",
    "import csv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils import try_makedirs\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "def ParseData(data_file):\n",
    "    ann = {}\n",
    "    if 'train' in data_file or 'validation' in data_file:\n",
    "        _ann = json.load(open(data_file))['annotations']\n",
    "        for a in _ann:\n",
    "            ann[a['image_id']] = a['label_id']\n",
    "\n",
    "    key_url_list = []\n",
    "    j = json.load(open(data_file))\n",
    "    images = j['images']\n",
    "    for item in images:\n",
    "        assert len(item['url']) == 1\n",
    "        url = item['url'][0]\n",
    "        id_ = item['image_id']\n",
    "        if id_ in ann:\n",
    "            id_ = (id_, ann[id_])\n",
    "        key_url_list.append((id_, url))\n",
    "    return key_url_list\n",
    "\n",
    "\n",
    "def DownloadImage(key_url):\n",
    "    out_dir = sys.argv[2]\n",
    "    (key, url) = key_url\n",
    "    if isinstance(key, tuple):\n",
    "        filename = os.path.join(out_dir, str(key[1]), '%s.jpg' % key[0])\n",
    "    else:\n",
    "        filename = os.path.join(out_dir, 'test', '%s.jpg' % key)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print('Image %s already exists. Skipping download.' % filename)\n",
    "        return\n",
    "    else:\n",
    "        try_makedirs(os.path.dirname(filename))\n",
    "\n",
    "    try:\n",
    "        # print('Trying to get %s.' % url)\n",
    "        http = urllib3.PoolManager()\n",
    "        response = http.request('GET', url, timeout=10)\n",
    "        image_data = response.data\n",
    "    except:\n",
    "        print('Warning: Could not download image %s from %s' % (os.path.basename(filename), url))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.open(BytesIO(image_data))\n",
    "    except:\n",
    "        print('Warning: Failed to parse image %s %s' % (os.path.basename(filename), url))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb = pil_image.convert('RGB')\n",
    "    except:\n",
    "        print('Warning: Failed to convert image %s to RGB' % os.path.basename(filename))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb.save(filename, format='JPEG', quality=90)\n",
    "    except:\n",
    "        print('Warning: Failed to save image %s' % filename)\n",
    "        return\n",
    "\n",
    "\n",
    "def Run():\n",
    "    if len(sys.argv) != 3:\n",
    "        print('Syntax: %s <train|validation|test.json> <output_dir/>' %\n",
    "              sys.argv[0])\n",
    "        sys.exit(0)\n",
    "    (data_file, out_dir) = sys.argv[1:]\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "    key_url_list = ParseData(data_file)\n",
    "    pool = multiprocessing.Pool(processes=80)\n",
    "\n",
    "    with tqdm(total=len(key_url_list)) as t:\n",
    "        for _ in pool.imap_unordered(DownloadImage, key_url_list):\n",
    "            t.update(1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):  # pylint: disable=R0903\n",
    "    \"\"\"Computes ROC AUC metrics\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()  # pylint: disable=E1003\n",
    "\n",
    "        self.x_val, self.y_val = validation_data\n",
    "        self.aucs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Count ROC AUC score at the end of each epoch\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            # for Sequentional models\n",
    "            y_pred = self.model.predict_proba(self.x_val, verbose=0)\n",
    "        else:\n",
    "            # for models that was created using functional API\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "        self.aucs.append(roc_auc_score(self.y_val, y_pred))\n",
    "        print(\n",
    "            '\\repoch: {:d} - ROC AUC: {:.6f}'.format(epoch + 1, self.aucs[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vgg16': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (224, 224),\n",
    "            'batch_size': 56\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 5\n",
    "        }\n",
    "    },\n",
    "    'skin_rec': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (141, 141),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 3\n",
    "        }\n",
    "    },\n",
    "     'lung_rec': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (141, 141),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'alex_net': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (141, 141),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'vgg19': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (224, 224),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'resnet50': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (224, 224),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'densenet': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (224, 224),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'incresnet': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (299, 299),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'incv3': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (141, 141),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    },\n",
    "    'xcept': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (299, 299),\n",
    "            'batch_size': 256\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 7\n",
    "        }\n",
    "    },\n",
    "    'nasnet': {\n",
    "        'flow_generator': {\n",
    "            'target_size': (331, 331),\n",
    "            'batch_size': 64\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'epochs': 15\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
